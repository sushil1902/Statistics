{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Statistics for Machine Learning",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhvS407EnDOS1HCv7I7dAQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushil1902/Statistics/blob/main/Statistics_for_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics for Machine Learning"
      ],
      "metadata": {
        "id": "kKe2yNiwDtO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **np.random.RandomState()** - a class that provides several methods based on different probability distributions.\n",
        "* **np.random.RandomState.seed()** - called when **RandomState()** is initialised."
      ],
      "metadata": {
        "id": "6mL1H_SvFG6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example below generates a sample of 100 random numbers drawn from a Gaussian distribution with a known mean of 50 and a standard deviation of 5 and calculates the summary statistics."
      ],
      "metadata": {
        "id": "6lcKSbtzFRVI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34iTCeKvDjBM",
        "outputId": "1775caa6-35c9-4b22-8fa7-dd914790a2c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 50.049\n",
            "Variance: 24.939\n",
            "Standard Deviation: 4.994\n"
          ]
        }
      ],
      "source": [
        "# calculate summary stats\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import mean\n",
        "from numpy import var\n",
        "from numpy import std\n",
        "# seed the random number generator\n",
        "seed(1)\n",
        "# generate univariate observations\n",
        "data = 5 * randn(10000) + 50\n",
        "# calculate statistics\n",
        "print('Mean: %.3f' % mean(data))\n",
        "print('Variance: %.3f' % var(data))\n",
        "print('Standard Deviation: %.3f' % std(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Between Variables\n",
        "\n",
        "Variables in a dataset may be related for lots of reasons.\n",
        "\n",
        "It can be useful in data analysis and modeling to better understand the relationships between variables. The statistical relationship between two variables is referred to as their correlation.\n",
        "\n",
        "A correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable’s value increases, the other variables’ values decrease.\n",
        "\n",
        "* Positive Correlation: Both variables change in the same direction.\n",
        "* Neutral Correlation: No relationship in the change of the variables.\n",
        "* Negative Correlation: Variables change in opposite directions.\n",
        "\n",
        "The performance of some algorithms can deteriorate if two or more variables are tightly related, called multicollinearity.\n",
        "\n",
        "An example is linear regression, where one of the offending correlated variables should be removed in order to improve the skill of the model.\n",
        "\n",
        "We can quantify the relationship between samples of two variables using a statistical method called Pearson’s correlation coefficient, named for the developer of the method, Karl Pearson.\n",
        "\n",
        "The **pearsonr()** NumPy function can be used to calculate the Pearson’s correlation coefficient for samples of two variables."
      ],
      "metadata": {
        "id": "tEKfQxerIsxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate correlation coefficient\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from scipy.stats import pearsonr\n",
        "# seed random number generator\n",
        "seed(1)\n",
        "# prepare data\n",
        "data1 = 20 * randn(1000) + 100\n",
        "data2 = data1 + (10 * randn(1000) + 50)\n",
        "# calculate Pearson's correlation\n",
        "corr, p = pearsonr(data1, data2)\n",
        "# display the correlation\n",
        "print('Pearsons correlation: %.3f' % corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GIrWVCGENns",
        "outputId": "3677bde4-939e-4eb0-f76e-8bae4787a4fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearsons correlation: 0.888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Hypothesis Tests\n",
        "\n",
        "Data must be interpreted in order to add meaning. We can interpret data by assuming a specific structure our outcome and use statistical methods to confirm or reject the assumption.\n",
        "\n",
        "The assumption is called a hypothesis and the statistical tests used for this purpose are called statistical hypothesis tests.\n",
        "\n",
        "The assumption of a statistical test is called the null hypothesis, or hypothesis zero (H0 for short). It is often called the default assumption, or the assumption that nothing has changed. \n",
        "\n",
        "A violation of the test’s assumption is often called the first hypothesis, hypothesis one, or H1 for short.\n",
        "\n",
        "* **Hypothesis 0 (H0)**: Assumption of the test holds and is failed to be rejected.\n",
        "* **Hypothesis 1 (H1)**: Assumption of the test does not hold and is rejected at some level of significance.\n",
        "\n",
        "We can interpret the result of a statistical hypothesis test using a p-value.\n",
        "\n",
        "The p-value is the probability of observing the data, given the null hypothesis is true.\n",
        "\n",
        "A large probability means that the H0 or default assumption is likely. \n",
        "\n",
        "A small value, such as below 5% (o.05) suggests that it is not likely and that we can reject H0 in favor of H1, or that something is likely to be different (e.g. a significant result).\n",
        "\n",
        "A widely used statistical hypothesis test is the Student’s t-test for comparing the mean values from two independent samples.\n",
        "\n",
        "The default assumption is that there is no difference between the samples, whereas a rejection of this assumption suggests some significant difference. \n",
        "\n",
        "The tests assumes that both samples were drawn from a Gaussian distribution and have the same variance.\n",
        "\n",
        "The Student’s t-test can be implemented in Python via the **ttest_ind()** SciPy function."
      ],
      "metadata": {
        "id": "pQCqpPr0Jx2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# student's t-test\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from scipy.stats import ttest_ind\n",
        "# seed the random number generator\n",
        "seed(1)\n",
        "# generate two independent samples\n",
        "data1 = 5 * randn(100) + 50\n",
        "data2 = 5 * randn(100) + 51\n",
        "# compare samples\n",
        "stat, p = ttest_ind(data1, data2)\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "# interpret\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('Same distributions (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Different distributions (reject H0)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07z_Dy8IJjqv",
        "outputId": "99ce39a1-f08c-41fc-8b81-cdab0297984d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics=-2.262, p=0.025\n",
            "Different distributions (reject H0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimation Statistics\n",
        "\n",
        "Statistical hypothesis tests can be used to indicate whether the difference between two samples is due to random chance, but cannot comment on the size of the difference.\n",
        "\n",
        "A group of methods referred to as “new statistics” are seeing increased use instead of or in addition to p-values in order to quantify the magnitude of effects and the amount of uncertainty for estimated values. This group of statistical methods is referred to as estimation statistics.\n",
        "\n",
        "Estimation statistics is a term to describe three main classes of methods. The three main\n",
        "classes of methods include:\n",
        "\n",
        "* **Effect Size**. Methods for quantifying the size of an effect given a treatment or intervention.\n",
        "* **Interval Estimation**. Methods for quantifying the amount of uncertainty in a value.\n",
        "* **Meta-Analysis**. Methods for quantifying the findings across multiple similar studies.\n",
        "\n",
        "Of the three, perhaps the most useful methods in applied machine learning are interval estimation.\n",
        "\n",
        "There are three main types of intervals. They are:\n",
        "\n",
        "* **Tolerance Interval**: The bounds or coverage of a proportion of a distribution with a specific level of confidence.\n",
        "* **Confidence Interval**: The bounds on the estimate of a population parameter.\n",
        "* **Prediction Interval**: The bounds on a single observation.\n",
        "\n",
        "A simple way to calculate a confidence interval for a classification algorithm is to calculate the binomial proportion confidence interval, which can provide an interval around a model’s estimated accuracy or error.\n",
        "\n",
        "This can be implemented in Python using the **confint() Statsmodels function**.\n",
        "\n",
        "The function takes the count of successes (or failures), the total number of trials, and the significance level as arguments and returns the lower and upper bound of the confidence interval.\n",
        "\n",
        "The example below demonstrates this function in a hypothetical case where a model made 88 correct predictions out of a dataset with 100 instances and we are interested in the 95% confidence interval (provided to the function as a significance of 0.05)."
      ],
      "metadata": {
        "id": "h3_JmBBmK3e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the confidence interval\n",
        "from statsmodels.stats.proportion import proportion_confint\n",
        "# calculate the interval\n",
        "lower, upper = proportion_confint(88, 100, 0.05)\n",
        "print('lower=%.3f, upper=%.3f' % (lower, upper))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SndeU13AKwXQ",
        "outputId": "0c3afa7d-8e1f-482e-d028-57f735bc7a65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lower=0.816, upper=0.944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nonparametric Statistics\n",
        "\n",
        "A large portion of the field of statistics and statistical methods is dedicated to data where the distribution is known.\n",
        "\n",
        "Data in which the distribution is unknown or cannot be easily identified is called nonparametric.\n",
        "\n",
        "In the case where you are working with nonparametric data, specialized nonparametric statistical methods can be used that discard all information about the distribution. \n",
        "\n",
        "As such, these methods are often referred to as distribution-free methods.\n",
        "\n",
        "Before a nonparametric statistical method can be applied, \n",
        "* the data must be converted into a rank format. \n",
        "* As such, statistical methods that expect data in rank format are sometimes called rank statistics, such as **rank correlation** and rank statistical hypothesis tests.\n",
        "* Ranking data is exactly as its name suggests.\n",
        "\n",
        "The procedure is as follows:\n",
        "\n",
        "Sort all data in the sample in ascending order.\n",
        "Assign an integer rank from 1 to N for each unique value in the data sample.\n",
        "A widely used nonparametric statistical hypothesis test for checking for a difference between two independent samples is the Mann-Whitney U test, named for Henry Mann and Donald Whitney.\n",
        "\n",
        "It is the nonparametric equivalent of the Student’s t-test but does not assume that the data is drawn from a Gaussian distribution.\n",
        "\n",
        "The test can be implemented in Python via the **mannwhitneyu() SciPy function**.\n",
        "\n",
        "The example below demonstrates the test on two data samples drawn from a uniform distribution known to be different."
      ],
      "metadata": {
        "id": "P4ap0erbMR4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# example of the mann-whitney u test\n",
        "from numpy.random import seed\n",
        "from numpy.random import rand\n",
        "from scipy.stats import mannwhitneyu\n",
        "# seed the random number generator\n",
        "seed(1)\n",
        "# generate two independent samples\n",
        "data1 = 50 + (rand(100) * 10)\n",
        "data2 = 51 + (rand(100) * 10)\n",
        "# compare samples\n",
        "stat, p = mannwhitneyu(data1, data2)\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "# interpret\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('Same distribution (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Different distribution (reject H0)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuP1XprHMGYC",
        "outputId": "f79c894a-82d8-4ba0-a391-87e0821be904"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics=4077.000, p=0.012\n",
            "Different distribution (reject H0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TrvB_sLUNDp0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}